{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e6c7532160b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"list-card-link\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0maddresses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0maddresses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext\n",
    "\n",
    "#set some display settings for notebooks\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'king-county,-wa_rb/' #*****change this city to what you want!!!!*****\n",
    "    \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
    "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
    "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
    "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
    "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
    "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
    "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
    "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
    "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
    "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "    \n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "\n",
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    \n",
    "    = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup1.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append first two dataframes\n",
    "df = df.append(df1, ignore_index = True) \n",
    "\n",
    "#create empty dataframes\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "df7 = pd.DataFrame()\n",
    "df8 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "for i in soup2:\n",
    "    soup = soup2\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df2['prices'] = price\n",
    "    df2['address'] = address\n",
    "    df2['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup2.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df2['links'] = urls\n",
    "df2['links'] = df2['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "    \n",
    "for i in soup3:\n",
    "    soup = soup3\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df3['prices'] = price1\n",
    "    df3['address'] = address1\n",
    "    df3['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup3.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df3['links'] = urls\n",
    "df3['links'] = df3['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup4:\n",
    "    soup = soup4\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df4['prices'] = price1\n",
    "    df4['address'] = address1\n",
    "    df4['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup4.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df4['links'] = urls\n",
    "df4['links'] = df4['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup5:\n",
    "    soup = soup5\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df5['prices'] = price1\n",
    "    df5['address'] = address1\n",
    "    df5['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup5.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df5['links'] = urls\n",
    "df5['links'] = df5['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup6:\n",
    "    soup = soup6\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df6['prices'] = price1\n",
    "    df6['address'] = address1\n",
    "    df6['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup6.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df6['links'] = urls\n",
    "df6['links'] = df6['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup7:\n",
    "    soup = soup7\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df7['prices'] = price1\n",
    "    df7['address'] = address1\n",
    "    df7['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup7.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df7['links'] = urls\n",
    "df7['links'] = df7['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "    \n",
    "for i in soup8:\n",
    "    soup = soup8\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df8['prices'] = price1\n",
    "    df8['address'] = address1\n",
    "    df8['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup8.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df8['links'] = urls\n",
    "df8['links'] = df8['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup9:\n",
    "    soup = soup9\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df9['prices'] = price1\n",
    "    df9['address'] = address1\n",
    "    df9['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup9.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df9['links'] = urls\n",
    "df9['links'] = df9['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "df = df.append(df2, ignore_index = True) \n",
    "df = df.append(df3, ignore_index = True) \n",
    "df = df.append(df4, ignore_index = True) \n",
    "df = df.append(df5, ignore_index = True) \n",
    "df = df.append(df6, ignore_index = True) \n",
    "df = df.append(df7, ignore_index = True) \n",
    "df = df.append(df8, ignore_index = True) \n",
    "df = df.append(df9, ignore_index = True) \n",
    "\n",
    "#convert columns to str\n",
    "df['prices'] = df['prices'].astype('str')\n",
    "df['address'] = df['address'].astype('str')\n",
    "df['beds'] = df['beds'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
    "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "#remove html tags from beds column\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
    "\n",
    "#split beds column into beds, bath and sq_feet\n",
    "df[['beds','baths','sq_feet']] = df.beds.str.split(expand=True)\n",
    "\n",
    "#remove commas from sq_feet and convert to float\n",
    "df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "#drop nulls\n",
    "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
    "\n",
    "#convert column to float\n",
    "df['prices'] = df['prices'].astype('float')\n",
    "# d['sq_feet'] = df['sq_feet'].astype('float')\n",
    "\n",
    "#remove spaces from link column\n",
    "df['links'] = df.links.str.replace(' ','')\n",
    "\n",
    "print('The column datatypes are:')\n",
    "print(df.dtypes)\n",
    "print('The dataframe shape is:', df.shape)\n",
    "\n",
    "#rearrange the columns\n",
    "df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
    "\n",
    "# df\n",
    "\n",
    "#calculate the zestimate and insert into a dataframe\n",
    "zillow_zestimate = []\n",
    "for link in df['links']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = soup.select_one('h4:contains(\"Home value\")')\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = home_value.find_next('p').get_text(strip=True)\n",
    "    zillow_zestimate.append(home_value)\n",
    "\n",
    "cols=['zestimate']\n",
    "zestimate_result = pd.DataFrame(zillow_zestimate, columns=cols)\n",
    "# zestimate_result\n",
    "\n",
    "#convert zestimate column to float, and remove , and $\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('$','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('/mo','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace(',','')\n",
    "\n",
    "#covert rows with non zestimate to 0\n",
    "def non_zestimate(zestimate_result):\n",
    "    if len(zestimate_result['zestimate']) > 20:\n",
    "        return '0'\n",
    "    elif len(zestimate_result['zestimate']) < 5:\n",
    "        return '0'\n",
    "    else:\n",
    "        return zestimate_result['zestimate']\n",
    "\n",
    "zestimate_result['zestimate'] = zestimate_result.apply(non_zestimate,axis=1)\n",
    "\n",
    "# zestimate_result\n",
    "\n",
    "#concat zestimate dataframe and original df\n",
    "df = pd.concat([df, zestimate_result], axis=1)\n",
    "df['zestimate'] = df['zestimate'].astype('float')\n",
    "\n",
    "#create best deal column and sort by best_deal\n",
    "df ['best_deal'] = df['prices'] - df['zestimate']\n",
    "df = df.sort_values(by='best_deal')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"12241 56th Pl S, Seattle, WA 98178\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/2eac3b97872525f2f443b93586451954-p_e.jpg\"/>]\n",
      "[<img alt=\"12240 SE 61st St, Bellevue, WA 98006\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/1a76e5cfe7e83e9aef0fb61c46156631-p_e.jpg\"/>]\n",
      "[<img alt=\"9737 S 239th Pl, Kent, WA 98031\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/da822c4202cb1e548c0252ac8e6b288d-p_e.jpg\"/>]\n",
      "[<img alt=\"37002 26th Pl S, Federal Way, WA 98003\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/079defe0cff2a9e8f785eb83a87670e9-p_e.jpg\"/>]\n",
      "[<img alt=\"9834 S 231st St, Kent, WA 98031\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/607e0cd82ac3b74ef8302ce9eb223959-p_e.jpg\"/>]\n",
      "[<img alt=\"405 Clay St, Auburn, WA 98001\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/18b9c43e13911919b9305eac63bd671a-p_e.jpg\"/>]\n",
      "[<img alt=\"27325 SE 306th St, Black Diamond, WA 98010\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/46233aa461f0a8879f471ae4595bb5dd-p_e.jpg\"/>]\n",
      "[<img alt=\"18610 Westside Hwy SW, Vashon, WA 98070\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/0968e83e5fd869a4eb9b7b66527b64f1-p_e.jpg\"/>]\n",
      "[<img alt=\"9614 NE 141st Pl, Kirkland, WA 98034\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/2795571741c25573f59945efa428b28c-p_e.jpg\"/>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-1cc57251851a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'list-card-link list-card-link-top-margin list-card-img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('article'):\n",
    "    href = link.find('a', class_ = 'list-card-link list-card-link-top-margin list-card-img')\n",
    "    print(href())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"list-card-info\"><a class=\"list-card-link list-card-link-top-margin\" href=\"https://www.zillow.com/homedetails/12241-56th-Pl-S-Seattle-WA-98178/48657043_zpid/\" tabindex=\"0\"></a><div class=\"list-card-footer\"></div><div class=\"list-card-heading\"><div class=\"list-card-price\">$385,000</div><ul class=\"list-card-details\"><li class=\"\">4<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li class=\"\">2<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">1,694<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- House for sale</li></ul></div></div>,\n",
       " <a class=\"list-card-link list-card-link-top-margin\" href=\"https://www.zillow.com/homedetails/12241-56th-Pl-S-Seattle-WA-98178/48657043_zpid/\" tabindex=\"0\"></a>,\n",
       " <div class=\"list-card-footer\"></div>,\n",
       " <div class=\"list-card-heading\"><div class=\"list-card-price\">$385,000</div><ul class=\"list-card-details\"><li class=\"\">4<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li class=\"\">2<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">1,694<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- House for sale</li></ul></div>,\n",
       " <div class=\"list-card-price\">$385,000</div>,\n",
       " <ul class=\"list-card-details\"><li class=\"\">4<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li class=\"\">2<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li class=\"\">1,694<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- House for sale</li></ul>,\n",
       " <li class=\"\">4<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li>,\n",
       " <abbr class=\"list-card-label\"> <!-- -->bds</abbr>,\n",
       " <li class=\"\">2<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li>,\n",
       " <abbr class=\"list-card-label\"> <!-- -->ba</abbr>,\n",
       " <li class=\"\">1,694<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li>,\n",
       " <abbr class=\"list-card-label\"> <!-- -->sqft</abbr>,\n",
       " <li class=\"list-card-statusText\">- House for sale</li>,\n",
       " <div class=\"list-card-top\"><div class=\"list-card-variable-text list-card-img-overlay\">Open: Fri. 3:30-6pm</div><div class=\"list-card-brokerage list-card-img-overlay provider-logo-overlay\"><img alt=\"\" aria-hidden=\"true\" class=\"provider-logo\" src=\"https://photos.zillowstatic.com/fp/6631fa4637aeea0746aac3f9507621d3-zillow_web_48_23.jpg\"/></div><a aria-hidden=\"false\" class=\"list-card-link list-card-link-top-margin list-card-img\" href=\"https://www.zillow.com/homedetails/12241-56th-Pl-S-Seattle-WA-98178/48657043_zpid/\" tabindex=\"-1\"><img alt=\"12241 56th Pl S, Seattle, WA 98178\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/2eac3b97872525f2f443b93586451954-p_e.jpg\"/></a></div>,\n",
       " <div class=\"list-card-variable-text list-card-img-overlay\">Open: Fri. 3:30-6pm</div>,\n",
       " <div class=\"list-card-brokerage list-card-img-overlay provider-logo-overlay\"><img alt=\"\" aria-hidden=\"true\" class=\"provider-logo\" src=\"https://photos.zillowstatic.com/fp/6631fa4637aeea0746aac3f9507621d3-zillow_web_48_23.jpg\"/></div>,\n",
       " <img alt=\"\" aria-hidden=\"true\" class=\"provider-logo\" src=\"https://photos.zillowstatic.com/fp/6631fa4637aeea0746aac3f9507621d3-zillow_web_48_23.jpg\"/>,\n",
       " <a aria-hidden=\"false\" class=\"list-card-link list-card-link-top-margin list-card-img\" href=\"https://www.zillow.com/homedetails/12241-56th-Pl-S-Seattle-WA-98178/48657043_zpid/\" tabindex=\"-1\"><img alt=\"12241 56th Pl S, Seattle, WA 98178\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/2eac3b97872525f2f443b93586451954-p_e.jpg\"/></a>,\n",
       " <img alt=\"12241 56th Pl S, Seattle, WA 98178\" aria-hidden=\"false\" class=\"\" src=\"https://photos.zillowstatic.com/fp/2eac3b97872525f2f443b93586451954-p_e.jpg\"/>,\n",
       " <div class=\"list-card-actions\"><button aria-label=\"Save\" class=\"list-card-save\" type=\"button\"><span class=\"list-card-save-content\" role=\"presentation\" tabindex=\"-1\"><svg focusable=\"false\" height=\"34\" viewbox=\"0 0 31 31\" width=\"34\" xmlns=\"http://www.w3.org/2000/svg\"><title>Save this home</title><path d=\"M18.5,0.00109769484 C22.0897727,0.00109769484 25,2.81119649 25,6.27991218 C25,8.06147091 24.2318182,9.66630077 22.9977273,10.8100988 L12.5,21 L1.8125,10.6256861 C0.690909091,9.49725576 0,7.96706915 0,6.27881449 C0,2.81119649 2.91022727,3.19744231e-14 6.5,3.19744231e-14 C9.20227273,3.19744231e-14 11.5193182,1.5949506 12.5,3.86388584 C13.4795455,1.5949506 15.7965909,0.00109769484 18.5,0.00109769484 L18.5,0.00109769484 Z\" fill=\"#000\" fill-opacity=\"0.2\" stroke=\"#FFF\" stroke-width=\"2\" transform=\"translate(3 3)\"></path></svg></span></button></div>,\n",
       " <button aria-label=\"Save\" class=\"list-card-save\" type=\"button\"><span class=\"list-card-save-content\" role=\"presentation\" tabindex=\"-1\"><svg focusable=\"false\" height=\"34\" viewbox=\"0 0 31 31\" width=\"34\" xmlns=\"http://www.w3.org/2000/svg\"><title>Save this home</title><path d=\"M18.5,0.00109769484 C22.0897727,0.00109769484 25,2.81119649 25,6.27991218 C25,8.06147091 24.2318182,9.66630077 22.9977273,10.8100988 L12.5,21 L1.8125,10.6256861 C0.690909091,9.49725576 0,7.96706915 0,6.27881449 C0,2.81119649 2.91022727,3.19744231e-14 6.5,3.19744231e-14 C9.20227273,3.19744231e-14 11.5193182,1.5949506 12.5,3.86388584 C13.4795455,1.5949506 15.7965909,0.00109769484 18.5,0.00109769484 L18.5,0.00109769484 Z\" fill=\"#000\" fill-opacity=\"0.2\" stroke=\"#FFF\" stroke-width=\"2\" transform=\"translate(3 3)\"></path></svg></span></button>,\n",
       " <span class=\"list-card-save-content\" role=\"presentation\" tabindex=\"-1\"><svg focusable=\"false\" height=\"34\" viewbox=\"0 0 31 31\" width=\"34\" xmlns=\"http://www.w3.org/2000/svg\"><title>Save this home</title><path d=\"M18.5,0.00109769484 C22.0897727,0.00109769484 25,2.81119649 25,6.27991218 C25,8.06147091 24.2318182,9.66630077 22.9977273,10.8100988 L12.5,21 L1.8125,10.6256861 C0.690909091,9.49725576 0,7.96706915 0,6.27881449 C0,2.81119649 2.91022727,3.19744231e-14 6.5,3.19744231e-14 C9.20227273,3.19744231e-14 11.5193182,1.5949506 12.5,3.86388584 C13.4795455,1.5949506 15.7965909,0.00109769484 18.5,0.00109769484 L18.5,0.00109769484 Z\" fill=\"#000\" fill-opacity=\"0.2\" stroke=\"#FFF\" stroke-width=\"2\" transform=\"translate(3 3)\"></path></svg></span>,\n",
       " <svg focusable=\"false\" height=\"34\" viewbox=\"0 0 31 31\" width=\"34\" xmlns=\"http://www.w3.org/2000/svg\"><title>Save this home</title><path d=\"M18.5,0.00109769484 C22.0897727,0.00109769484 25,2.81119649 25,6.27991218 C25,8.06147091 24.2318182,9.66630077 22.9977273,10.8100988 L12.5,21 L1.8125,10.6256861 C0.690909091,9.49725576 0,7.96706915 0,6.27881449 C0,2.81119649 2.91022727,3.19744231e-14 6.5,3.19744231e-14 C9.20227273,3.19744231e-14 11.5193182,1.5949506 12.5,3.86388584 C13.4795455,1.5949506 15.7965909,0.00109769484 18.5,0.00109769484 L18.5,0.00109769484 Z\" fill=\"#000\" fill-opacity=\"0.2\" stroke=\"#FFF\" stroke-width=\"2\" transform=\"translate(3 3)\"></path></svg>,\n",
       " <title>Save this home</title>,\n",
       " <path d=\"M18.5,0.00109769484 C22.0897727,0.00109769484 25,2.81119649 25,6.27991218 C25,8.06147091 24.2318182,9.66630077 22.9977273,10.8100988 L12.5,21 L1.8125,10.6256861 C0.690909091,9.49725576 0,7.96706915 0,6.27881449 C0,2.81119649 2.91022727,3.19744231e-14 6.5,3.19744231e-14 C9.20227273,3.19744231e-14 11.5193182,1.5949506 12.5,3.86388584 C13.4795455,1.5949506 15.7965909,0.00109769484 18.5,0.00109769484 L18.5,0.00109769484 Z\" fill=\"#000\" fill-opacity=\"0.2\" stroke=\"#FFF\" stroke-width=\"2\" transform=\"translate(3 3)\"></path>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.article.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
